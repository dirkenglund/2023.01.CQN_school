{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dirkenglund/2023.01.CQN_school/blob/main/MIWEN_exact_mixer_with_carrier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1r2l-PmcL7Ms"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time, os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from scipy.constants import speed_of_light as c\n",
        "import scipy.io\n",
        "from collections import OrderedDict\n",
        "import torch.fft as tfft\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# gpu = torch.device(\"mps\")\n",
        "# gpu = torch.device(\"cpu\")\n",
        "\n",
        "# ## Sets everything to double point precision (use with gradcheck)\n",
        "# torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTPcuztQR6FN",
        "outputId": "2f037919-8fb8-44ae-8f54-8cce9e604fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available: Tesla T4\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2FZcu0MUQnDq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mutual_info_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HMxNfJQnQnLc"
      },
      "outputs": [],
      "source": [
        "speedoflight = 3e8\n",
        "epsilon0 = 8.85e-12\n",
        "\n",
        "kb = 1.38e-23; T = 300\n",
        "hbar = 1.05e-34\n",
        "\n",
        "eleccharge = 1.6e-19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "loEpiOo8QnOC"
      },
      "outputs": [],
      "source": [
        "def boseeinstein(omega, T):\n",
        "\n",
        "    return 1/(torch.exp(hbar*omega/(kb*T))-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ9YTGvDQnQY",
        "outputId": "67f1b2b3-313c-4dfc-9422-ef0ff47b9af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 490kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.48MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 12.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# A list of transforms to perform on all image data\n",
        "# First turn image data into pytorch tensor, then normalize it with mean and std of 0.5\n",
        "# (only one channel per image) since there's only one element in each tuple\n",
        "\n",
        "# img_size = (14, 14)\n",
        "img_size = (7, 7)\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.Resize(img_size, interpolation=2), torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# transform = torchvision.transforms.Compose(\n",
        "#     [torchvision.transforms.ToTensor(),\n",
        "#      torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Downloading the training MNIST data and applying the above transform\n",
        "# (The data type is a dataset, iterated by 60000 tuples, where the first element\n",
        "# of each tuple contains the tensor with the data, and the second element is an integer)\n",
        "# Note: each image in both the trainset and testset have shape [1, 28, 28]\n",
        "dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "num_training = 50000\n",
        "num_validation = 10000 # 10000\n",
        "random_numbers = np.random.choice(np.arange(60000), size=num_training+num_validation, replace=False)\n",
        "\n",
        "# Convert the NumPy range to a Python list\n",
        "subset_indices = list(random_numbers)\n",
        "\n",
        "# Create the subset using torch.utils.data.Subset\n",
        "subset_dataset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "\n",
        "trainset, validset = torch.utils.data.random_split(subset_dataset, [num_training, num_validation])\n",
        "\n",
        "# Downloading the test MNIST data and applying the above transform\n",
        "# (The data type is a dataset, iterated by 10000 tuples, where the first element\n",
        "# of each tuple contains the tensor with the data, and the second element is an integer)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "num_test = 10000 # 10000\n",
        "random_numbers2 = np.random.choice(np.arange(10000), size=num_test, replace=False)\n",
        "\n",
        "# Convert the NumPy range to a Python list\n",
        "subset_indices2 = list(random_numbers2)\n",
        "\n",
        "# Create the subset using torch.utils.data.Subset\n",
        "subset_testset = torch.utils.data.Subset(testset, subset_indices2)\n",
        "\n",
        "# The loaded sets of data, ready to be inputted into a neural net.\n",
        "# Each \"loader\" splits all the data into specified batches.  When iterated, there are\n",
        "# (num of feature maps / num of batches) items, and each item contains a list of length 2,\n",
        "# where the first element contains a tensor of (num of batches) elements containing the tensors of data,\n",
        "# and the second element contains a tensor of (num of batches) elements containing the tensors of labels\n",
        "batch_size = 64\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(subset_testset, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Z0vay52QnSw"
      },
      "outputs": [],
      "source": [
        "num_test3 = 1000 # 10000\n",
        "random_numbers3 = np.random.choice(np.arange(10000), size=num_test3, replace=False)\n",
        "\n",
        "# Convert the NumPy range to a Python list\n",
        "subset_indices3 = list(random_numbers3)\n",
        "\n",
        "# Create the subset using torch.utils.data.Subset\n",
        "subset_testset3 = torch.utils.data.Subset(testset, subset_indices3)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "testloader1000 = torch.utils.data.DataLoader(subset_testset3, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "69Wy81msQnVW"
      },
      "outputs": [],
      "source": [
        "# Sum scaling function\n",
        "def sum_scaling(logits):\n",
        "    logits = torch.exp(logits)\n",
        "    return logits / logits.sum(dim=1, keepdim=True)+1e-10\n",
        "#     return logits / logits.sum(dim=1, keepdim=True)\n",
        "\n",
        "# Custom cross-entropy loss function\n",
        "def custom_cross_entropy_loss(probabilities, targets):\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "    nll_loss = F.nll_loss(log_probabilities, targets)\n",
        "    return nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UZhOycYoQnYT"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(loader, model, device, printprogress=False):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for images, labels in loader:\n",
        "            # Move the input and labels to the same device as the model\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Get the predicted class with the highest score\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Calculate total and correct predictions\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if printprogress:\n",
        "                print(f\"{total} test batches processed\")\n",
        "\n",
        "    model.train()  # Set the model back to training mode\n",
        "    return 100 * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISELnPhjQna1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vYPm2NDdQndq"
      },
      "outputs": [],
      "source": [
        "class Params:\n",
        "    def __init__(self, red=True, units=1, fixedband=False,\n",
        "                 totBW=None, noisein=False, noiseout=False, lowtemp=False,\n",
        "                 weightfreqspacing=None, k_th_AWG=None, k_th_AWG_lowtemp=None,\n",
        "                 k_th_AWG_lowtemp_opt=None, k_th_w=None, k_th_out=None,\n",
        "                 k_th_w_lowtemp=None, k_th_out_lowtemp=None,\n",
        "                 k_th_A2D=None, k_th_A2D_lowtemp=None,\n",
        "                 digitallike=False, noisefactor=4, optics=False,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=1, unitconverter=1,\n",
        "                 photodiodearea=1e-6, omegaoptical=2*torch.pi*193.41e12,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=True, resconn=False,\n",
        "                 t=None, del_w=None, w_carrier=None, w_carrier_wei=None):\n",
        "\n",
        "        self.red = red\n",
        "        self.units = units\n",
        "        self.fixedband = fixedband\n",
        "        self.totBW = totBW\n",
        "        self.noisein = noisein\n",
        "        self.noiseout = noiseout\n",
        "        self.lowtemp = lowtemp\n",
        "        self.weightfreqspacing = weightfreqspacing\n",
        "        self.k_th_AWG = k_th_AWG\n",
        "        self.k_th_AWG_lowtemp = k_th_AWG_lowtemp\n",
        "        self.k_th_AWG_lowtemp_opt = k_th_AWG_lowtemp_opt\n",
        "        self.k_th_w = k_th_w\n",
        "        self.k_th_out = k_th_out\n",
        "        self.k_th_w_lowtemp = k_th_w_lowtemp\n",
        "        self.k_th_out_lowtemp = k_th_out_lowtemp\n",
        "        self.k_th_A2D = k_th_A2D\n",
        "        self.k_th_A2D_lowtemp = k_th_A2D_lowtemp\n",
        "        self.digitallike = digitallike\n",
        "        self.noisefactor = noisefactor\n",
        "        self.optics = optics\n",
        "        self.fixedpowerflag = fixedpowerflag\n",
        "        self.fixedpower = fixedpower\n",
        "        self.includeinputfft = includeinputfft\n",
        "        self.includeoutputfft = includeoutputfft\n",
        "        self.scaletoz = scaletoz\n",
        "        self.scaletoW = scaletoW\n",
        "        self.wunits = wunits\n",
        "        self.unitconverter = unitconverter\n",
        "        self.photodiodearea = photodiodearea\n",
        "        self.omegaoptical = omegaoptical\n",
        "        self.inttime = inttime\n",
        "        self.capval = capval\n",
        "        self.semianalog = semianalog\n",
        "        self.resconn = resconn\n",
        "\n",
        "        self.t = t\n",
        "        self.del_w = del_w\n",
        "        self.w_carrier = w_carrier\n",
        "        self.w_carrier_wei = w_carrier_wei\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XJRgFipNQngB"
      },
      "outputs": [],
      "source": [
        "class ActivationEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, inputsize, outputsize):\n",
        "        super(ActivationEncoding, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "        self.N = inputsize\n",
        "        self.R = outputsize\n",
        "\n",
        "        self.del_w = params.del_w\n",
        "        self.w_carrier = params.w_carrier\n",
        "\n",
        "        if self.params.red:\n",
        "            self.n0 = 0\n",
        "            self.r0 = self.R * (self.N - 1) / 2\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) + (self.n0 + self.N) * self.R) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "        else:\n",
        "            self.n0 = int(self.N * (self.R - 1) / 2)\n",
        "            self.r0 = 0\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) * self.N + (self.n0 + self.N)) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device  # Get the device from the input tensor\n",
        "\n",
        "        batchsize, numpixels = x.shape\n",
        "\n",
        "        # Generate the n values (1 to N) for the exponential term, on the same device as input\n",
        "        n_values = torch.arange(1, numpixels + 1, device=device).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, N)\n",
        "\n",
        "        # Reshape the time vector to allow broadcasting in the computation, move to the input's device\n",
        "        t = self.params.t.to(device).unsqueeze(1).unsqueeze(2)  # Shape (n_t, 1, 1)\n",
        "\n",
        "        # Compute the exponential term with broadcasting, ensuring all computations are on the same device\n",
        "        if self.params.red:\n",
        "            exp_term = torch.exp(1j * ((self.n0 + n_values) * self.R * self.del_w + self.w_carrier) * t)  # Shape (n_t, 1, N)\n",
        "        else:\n",
        "            exp_term = torch.exp(1j * ((self.n0 + n_values) * self.del_w + self.w_carrier) * t)  # Shape (n_t, 1, N)\n",
        "\n",
        "        # Expand the input tensor to match the shape of the exponential term\n",
        "        x_expanded = x.unsqueeze(0)  # Shape (1, batch_size, N)\n",
        "\n",
        "        # Perform element-wise multiplication and summation over the second dimension (N)\n",
        "        result = torch.sum(x_expanded * exp_term, dim=2)  # Shape (n_t, batch_size)\n",
        "\n",
        "        # Transpose the result to get shape (batch_size, n_t), and take the real part\n",
        "        x = torch.real(result.transpose(0, 1))  # Shape (batch_size, n_t)\n",
        "\n",
        "        if self.params.includeinputfft:\n",
        "            x_complex = torch.complex(x, torch.zeros_like(x, device=device))  # Ensure zeros are on the correct device\n",
        "            x = torch.fft.fft(x_complex.conj().t(), dim=0, norm=\"ortho\").conj().t()\n",
        "\n",
        "        if self.params.red:\n",
        "            # inputs_extended = x.unsqueeze(2)\n",
        "            # extendedshape = inputs_extended.shape\n",
        "            # zeropaddingleft = torch.zeros(extendedshape[0], extendedshape[1], self.R - 1, device=device)\n",
        "            # output_inter_tensor = torch.cat([zeropaddingleft, inputs_extended], dim=-1)\n",
        "            # output_inter_tensor = output_inter_tensor.view(extendedshape[0], -1)\n",
        "            # furtherzeropaddingleft = torch.zeros(extendedshape[0], self.n0 * self.R + 1, device=device)\n",
        "            # output_inter_tensor = torch.cat([furtherzeropaddingleft, output_inter_tensor], dim=-1)\n",
        "            # zeropaddingright = torch.zeros(extendedshape[0], self.highestindex - output_inter_tensor.shape[-1], device=device)\n",
        "            # y = torch.cat([output_inter_tensor, zeropaddingright], dim=-1)\n",
        "            # # x = torch.real(torch.fft.irfft(y, dim=-1, norm=\"forward\"))\n",
        "            # x = torch.real(torch.fft.irfft(y, dim=-1, norm=\"ortho\"))\n",
        "\n",
        "            if self.params.fixedpowerflag:\n",
        "                avgpowx = torch.mean(x ** 2, dim=-1, keepdim=True) / 50\n",
        "                multfact = self.params.fixedpower / avgpowx\n",
        "                x = x * multfact\n",
        "\n",
        "            if not self.params.optics:\n",
        "                if not self.params.fixedband:\n",
        "                    xener = torch.sum(x ** 2, dim=-1) * (1 / self.params.weightfreqspacing) / self.total\n",
        "                else:\n",
        "                    xener = torch.sum(x ** 2, dim=-1) * (1 / self.params.totBW)\n",
        "            else:\n",
        "                xener = torch.sum(x ** 2, dim=-1)\n",
        "        else:\n",
        "            # xshape = x.shape\n",
        "            # zeropaddingleft = torch.zeros(xshape[0], self.n0 + 1, device=device)\n",
        "            # output_inter_tensor = torch.cat([zeropaddingleft, x], dim=-1)\n",
        "            # zeropaddingright = torch.zeros(xshape[0], self.highestindex - output_inter_tensor.shape[-1], device=device)\n",
        "            # y = torch.cat([output_inter_tensor, zeropaddingright], dim=-1)\n",
        "            # # x = torch.real(torch.fft.irfft(y, dim=-1, norm=\"forward\"))\n",
        "            # x = torch.real(torch.fft.irfft(y, dim=-1, norm=\"ortho\"))\n",
        "\n",
        "            if self.params.fixedpowerflag:\n",
        "                avgpowx = torch.mean(x ** 2, dim=-1, keepdim=True) / 50\n",
        "                multfact = self.params.fixedpower / avgpowx\n",
        "                x = x * multfact\n",
        "\n",
        "            if not self.params.optics:\n",
        "                if not self.params.fixedband:\n",
        "                    xener = torch.sum(x ** 2, dim=-1) * (1 / self.params.weightfreqspacing) / self.total\n",
        "                else:\n",
        "                    xener = torch.sum(x ** 2, dim=-1) * (1 / self.params.totBW)\n",
        "            else:\n",
        "                xener = torch.sum(x ** 2, dim=-1)\n",
        "\n",
        "        # Adding noise in the time domain\n",
        "        if self.params.noisein:\n",
        "            if not self.params.optics:\n",
        "                noisetouse = self.params.k_th_AWG_lowtemp if self.params.lowtemp else self.params.k_th_AWG\n",
        "                sigma2_x_th = noisetouse * self.params.weightfreqspacing * self.total if not self.params.fixedband else noisetouse * self.params.totBW\n",
        "                if self.params.digitallike:\n",
        "                    sigma2_x_th = sigma2_x_th * self.params.noisefactor\n",
        "                xadditivenoise = torch.normal(0., torch.sqrt(sigma2_x_th), size=x.shape, device=device)\n",
        "                x = x + xadditivenoise\n",
        "\n",
        "        return x, xener\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HEAqOoblRQoz"
      },
      "outputs": [],
      "source": [
        "class WeightEncodingandMixing(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, inputsize, outputsize, actshape=None):\n",
        "        super(WeightEncodingandMixing, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "        self.N = inputsize\n",
        "        self.R = outputsize\n",
        "        self.actshape = actshape\n",
        "\n",
        "        self.del_w = params.del_w\n",
        "        self.w_carrier_wei = params.w_carrier_wei\n",
        "\n",
        "        if self.params.red:\n",
        "            self.n0 = 0\n",
        "            self.r0 = self.R * (self.N - 1) / 2\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) + (self.n0 + self.N) * self.R) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "        else:\n",
        "            self.n0 = int(self.N * (self.R - 1) / 2)\n",
        "            self.r0 = 0\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) * self.N + (self.n0 + self.N)) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "\n",
        "        if self.params.semianalog:\n",
        "            self.W = nn.Parameter(torch.empty(outputsize, inputsize))\n",
        "            nn.init.kaiming_uniform_(self.W, a=0)\n",
        "        else:\n",
        "            self.W = nn.Parameter(torch.empty(outputsize, inputsize))\n",
        "            # self.W = nn.Parameter(torch.empty(int(1), int(self.actshape[1] / 2 + 1)))\n",
        "            nn.init.kaiming_uniform_(self.W, a=0)\n",
        "\n",
        "    def forward(self, z):\n",
        "        device = z.device  # Get the device from the input tensor\n",
        "\n",
        "        if self.params.semianalog:\n",
        "            if self.params.red:\n",
        "                if not self.params.includeinputfft and not self.params.includeoutputfft:\n",
        "                    Wtr = self.W.t()\n",
        "                else:\n",
        "                    selfW = self.W.to(dtype=torch.complex64)\n",
        "\n",
        "                    if self.params.includeoutputfft:\n",
        "                        selfW = torch.fft.fft(selfW, dim=0, norm=\"ortho\")\n",
        "\n",
        "                    if self.params.includeinputfft:\n",
        "                        selfW = torch.fft.fft(selfW.conj().t(), dim=0, norm=\"ortho\").conj().t()\n",
        "\n",
        "                    Wtr = selfW.t()\n",
        "\n",
        "                Wunrolled = Wtr.reshape(-1)\n",
        "                zeropaddingleft = torch.zeros(int(self.r0 + 1 + (self.n0 + 1) * self.R), device=device)\n",
        "                Wpaddedleft = torch.cat([zeropaddingleft, Wunrolled], dim=-1)\n",
        "                zeropaddingright = torch.zeros(self.highestindex - Wpaddedleft.shape[-1], device=device)\n",
        "                Wpadded = torch.cat([Wpaddedleft, zeropaddingright], dim=-1)\n",
        "                # Wtime = torch.real(torch.fft.irfft(Wpadded, dim=-1, norm=\"forward\").unsqueeze(0))\n",
        "                Wtime = torch.real(torch.fft.irfft(Wpadded, dim=-1, norm=\"ortho\").unsqueeze(0))\n",
        "            else:\n",
        "                Wunrolled = self.W.reshape(-1)\n",
        "                zeropaddingleft = torch.zeros(int((self.r0 + 1) * self.N + (self.n0 + 1)), device=device)\n",
        "                Wpaddedleft = torch.cat([zeropaddingleft, Wunrolled], dim=-1)\n",
        "                zeropaddingright = torch.zeros(self.highestindex - Wpaddedleft.shape[-1], device=device)\n",
        "                Wpadded = torch.cat([Wpaddedleft, zeropaddingright], dim=-1)\n",
        "                # Wtime = torch.real(torch.fft.irfft(Wpadded, dim=-1, norm=\"forward\").unsqueeze(0))\n",
        "                Wtime = torch.real(torch.fft.irfft(Wpadded, dim=-1, norm=\"ortho\").unsqueeze(0))\n",
        "\n",
        "            if self.params.noisein:\n",
        "                if not self.params.optics:\n",
        "                    noisetouse = self.params.k_th_w_lowtemp if self.params.lowtemp else self.params.k_th_w\n",
        "                    sigma2_W_th = noisetouse * self.params.weightfreqspacing * self.total if not self.params.fixedband else noisetouse * self.params.totBW\n",
        "                    if self.params.digitallike:\n",
        "                        sigma2_W_th = sigma2_W_th * self.params.noisefactor\n",
        "                    Wadditivenoise = torch.normal(0., torch.sqrt(sigma2_W_th), size=Wtime.shape, device=device)\n",
        "                    Wtime = Wtime + Wadditivenoise\n",
        "        else:\n",
        "\n",
        "            # Define the time vector, move it to the correct device, and reshape\n",
        "            t = self.params.t.to(device).unsqueeze(1).unsqueeze(2)  # Shape (n_t, 1, 1)\n",
        "\n",
        "            # Define the 2D tensor W_rn with shape (R, N), then expand to 3D\n",
        "            W_rn = self.W.to(device).unsqueeze(0)  # Shape (1, R, N)\n",
        "\n",
        "            # Generate r and n indices and expand to match dimensions, ensure they are on the correct device\n",
        "            r_values = torch.arange(1, self.R + 1, device=device).view(1, self.R, 1)  # Shape (1, R, 1)\n",
        "            n_values = torch.arange(1, self.N + 1, device=device).view(1, 1, self.N)  # Shape (1, 1, N)\n",
        "\n",
        "            # Compute ω^W_{r,n} as a 3D tensor on the same device\n",
        "            if self.params.red:\n",
        "                w_offset_weights = (self.r0 + r_values) * self.del_w + (self.n0 + n_values) * self.R * self.del_w  # Shape (1, R, N)\n",
        "            else:\n",
        "                w_offset_weights = (self.r0 + r_values) * self.R * self.del_w + (self.n0 + n_values) * self.del_w\n",
        "\n",
        "            # Compute the exponential term with broadcasting over time\n",
        "            exp_term = torch.exp(1j * (w_offset_weights + self.w_carrier_wei) * t)  # Shape (n_t, R, N)\n",
        "\n",
        "            # Perform element-wise multiplication and summation over r and n dimensions\n",
        "            result = torch.sum(W_rn * exp_term, dim=(1, 2))  # Shape (n_t,)\n",
        "\n",
        "            # Unsqueeze the result to get shape (1, n_t), and take the real part\n",
        "            Wtime = torch.real(result.unsqueeze(0))  # Shape (1, n_t)\n",
        "\n",
        "            # Wtime = torch.real(torch.fft.irfft(self.W, dim=-1, norm=\"forward\"))\n",
        "            # Wtime = torch.real(torch.fft.irfft(self.W, dim=-1, norm=\"ortho\"))\n",
        "\n",
        "        if not self.params.optics:\n",
        "            # print(f\"z device is {z.device}\")\n",
        "            # print(f\"Wtime device is {Wtime.device}\")\n",
        "\n",
        "            # small signal multiplication --> amplitudes get killed\n",
        "            print(f\"z shape is {z.shape}\")\n",
        "            print(f\"Wtime shape is {Wtime.shape}\")\n",
        "            outputs_in_time = (1/(4*(kb*T/eleccharge))) * torch.mul(z, Wtime)\n",
        "            # outputs_in_time = torch.mul(torch.sign(z), Wtime)\n",
        "            # outputs_in_time = torch.mul(z, torch.sign(Wtime))\n",
        "\n",
        "            # exact formula (non diff?)\n",
        "            # VT = kb*T/eleccharge\n",
        "            # outputs_in_time = Wtime/2 + (VT/2)*torch.log((torch.exp(z/VT)+torch.exp(-Wtime/VT))/(torch.exp(z/VT)+torch.exp(Wtime/VT)))\n",
        "        else:\n",
        "            upper_out = (z + Wtime) / torch.sqrt(torch.tensor(2, device=device))\n",
        "            lower_out = (z - Wtime) / torch.sqrt(torch.tensor(2, device=device))\n",
        "            nth = boseeinstein(self.params.omegaoptical, T)\n",
        "            upper_var = (nth + nth**2 + (2 * nth + 1) * upper_out**2)\n",
        "            lower_var = (nth + nth**2 + (2 * nth + 1) * lower_out**2)\n",
        "            diff_current = eleccharge * torch.normal(mean=2 * z * Wtime, std=torch.sqrt(upper_var + lower_var), device=device)\n",
        "            outputs_in_time = diff_current / self.params.capval\n",
        "\n",
        "        if not self.params.optics and self.params.scaletoz:\n",
        "            outputs_in_time = F.normalize(outputs_in_time, p=2, dim=-1)\n",
        "            znorm = torch.norm(z, p=2, dim=-1, keepdim=True)\n",
        "            outputs_in_time = torch.mul(znorm, outputs_in_time)\n",
        "\n",
        "        if not self.params.optics and self.params.scaletoW:\n",
        "            outputs_in_time = F.normalize(outputs_in_time, p=2, dim=-1)\n",
        "            Wnorm = torch.norm(Wtime, p=2, dim=-1, keepdim=True)\n",
        "            outputs_in_time = torch.mul(Wnorm, outputs_in_time)\n",
        "\n",
        "        outputs_in_time = outputs_in_time * self.params.unitconverter\n",
        "\n",
        "        if self.params.noiseout:\n",
        "            noisetouse = self.params.k_th_out_lowtemp if self.params.lowtemp else self.params.k_th_out\n",
        "            sigma2_output_th = noisetouse * self.params.weightfreqspacing * self.total if not self.params.fixedband else noisetouse * self.params.totBW\n",
        "            if not self.params.digitallike:\n",
        "                output_additivenoise = torch.normal(0., torch.sqrt(sigma2_output_th), size=Wtime.shape, device=device)\n",
        "                outputs_in_time = outputs_in_time + output_additivenoise\n",
        "\n",
        "        return outputs_in_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MR9m0Ub5RQrI"
      },
      "outputs": [],
      "source": [
        "class A2D(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, inputsize, outputsize):\n",
        "        super(A2D, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "\n",
        "        # if self.params.noiseout:\n",
        "        self.N = inputsize\n",
        "        self.R = outputsize\n",
        "\n",
        "        if self.params.red:\n",
        "            self.n0 = 0\n",
        "            self.r0 = self.R * (self.N - 1) / 2\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) + (self.n0 + self.N) * self.R) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "        else:\n",
        "            self.n0 = int(self.N * (self.R - 1) / 2)\n",
        "            self.r0 = 0\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) * self.N + (self.n0 + self.N)) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "\n",
        "    def forward(self, outputs_in_time):\n",
        "        device = outputs_in_time.device  # Get the device of the input tensor\n",
        "\n",
        "        if self.params.noiseout:\n",
        "            noisetouse = self.params.k_th_A2D_lowtemp if self.params.lowtemp else self.params.k_th_A2D\n",
        "\n",
        "            if not self.params.fixedband:\n",
        "                sigma2_A2D_th = noisetouse * self.params.weightfreqspacing * self.total\n",
        "            else:\n",
        "                sigma2_A2D_th = noisetouse * self.params.totBW\n",
        "\n",
        "            if self.params.semianalog:\n",
        "                sigma2_A2D_th = sigma2_A2D_th * self.params.noisefactor\n",
        "\n",
        "            if not self.params.digitallike:\n",
        "                additivenoise = torch.normal(0., torch.sqrt(sigma2_A2D_th), size=outputs_in_time.shape, device=device)\n",
        "                outputs_in_time = outputs_in_time + additivenoise\n",
        "\n",
        "        if self.params.optics:\n",
        "            outputs_in_time = outputs_in_time * self.params.capval / (2 * eleccharge)\n",
        "        else:\n",
        "            outputs_in_time = outputs_in_time * 4 * (kb*T/eleccharge)\n",
        "\n",
        "        # Perform the FFT on the same device\n",
        "        # outputs_in_freq = torch.fft.rfft(outputs_in_time, dim=-1, norm=\"forward\")\n",
        "        # outputs_in_freq = torch.sqrt(torch.tensor(self.total)) * torch.fft.rfft(outputs_in_time, dim=-1, norm=\"ortho\")\n",
        "\n",
        "        # to get things to work for the time-domain carrier stuff\n",
        "        outputs_in_freq = torch.sqrt(torch.tensor(self.total)) * torch.real(torch.fft.rfft(outputs_in_time, dim=-1, norm=\"ortho\"))\n",
        "\n",
        "        return outputs_in_freq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MAFTFilter_TimeOutput(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, inputsize, outputsize):\n",
        "        super(MAFTFilter_TimeOutput, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "        self.N = inputsize\n",
        "        self.R = outputsize\n",
        "\n",
        "        if self.params.red:\n",
        "            self.n0 = 0\n",
        "            self.r0 = self.R * (self.N - 1) / 2\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) + (self.n0 + self.N) * self.R) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "        else:\n",
        "            self.n0 = int(self.N * (self.R - 1) / 2)\n",
        "            self.r0 = 0\n",
        "            self.highestindex = int(2 * ((self.r0 + self.R) * self.N + (self.n0 + self.N)) + 4)\n",
        "            self.total = int(2 * (self.highestindex - 1))\n",
        "\n",
        "    def forward(self, z):\n",
        "        device = z.device  # Get the device of the input tensor\n",
        "\n",
        "        if self.params.red:\n",
        "            z[:, :int(self.r0 + 1)] = 0\n",
        "            z[:, int(self.r0 + self.R + 1):] = 0\n",
        "            outputs_in_time = torch.fft.irfft(z, dim=-1, norm=\"ortho\") / torch.sqrt(torch.tensor(self.total))\n",
        "            # outputs_in_freq = torch.sqrt(torch.tensor(self.total)) * torch.fft.rfft(outputs_in_time, dim=-1, norm=\"ortho\")\n",
        "            # z = z[:, int(self.r0 + 1):int(self.r0 + self.R + 1)]\n",
        "        # else:\n",
        "        #     z = z[:, int((self.r0 + 1) * self.N):int((self.r0 + self.R) * self.N + 1):int(self.N)]\n",
        "\n",
        "        # if self.params.includeoutputfft:\n",
        "        #     z = torch.fft.ifft(z.t(), dim=0, norm=\"ortho\").t().real\n",
        "\n",
        "        if self.params.optics:\n",
        "            outputs_in_time = outputs_in_time / (self.params.capval / (2 * eleccharge))\n",
        "        else:\n",
        "            outputs_in_time = outputs_in_time / (4 * (kb*T/eleccharge))\n",
        "\n",
        "        return outputs_in_time.real\n"
      ],
      "metadata": {
        "id": "I-b1B2e3MN2u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g0_fsu_MRQtO"
      },
      "outputs": [],
      "source": [
        "class MAFTFilter(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, inputsize, outputsize):\n",
        "        super(MAFTFilter, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "        self.N = inputsize\n",
        "        self.R = outputsize\n",
        "\n",
        "        if self.params.red:\n",
        "            self.r0 = self.R * (self.N - 1) / 2\n",
        "        else:\n",
        "            self.r0 = 0\n",
        "\n",
        "    def forward(self, z):\n",
        "        device = z.device  # Get the device of the input tensor\n",
        "\n",
        "        if self.params.red:\n",
        "            z = z[:, int(self.r0 + 1):int(self.r0 + self.R + 1)]\n",
        "        else:\n",
        "            z = z[:, int((self.r0 + 1) * self.N):int((self.r0 + self.R) * self.N + 1):int(self.N)]\n",
        "\n",
        "        if self.params.includeoutputfft:\n",
        "            z = torch.fft.ifft(z.t(), dim=0, norm=\"ortho\").t().real\n",
        "\n",
        "        return z.real\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GQ0wpQ_8RQvk"
      },
      "outputs": [],
      "source": [
        "class SquareNonlinorig(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(SquareNonlinorig, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        z = torch.square(z)\n",
        "        z = z/torch.max(z, dim=1, keepdim=True).values\n",
        "\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "18NeKKMIRQyK"
      },
      "outputs": [],
      "source": [
        "class SquareNonlin(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(SquareNonlin, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        z = (1/(4*(kb*T/eleccharge))) * torch.square(z)/2\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kfY_g3GMRc2C"
      },
      "outputs": [],
      "source": [
        "class MyNonlin(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(MyNonlin, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        z = torch.nn.functional.relu(z)\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hnIobGQPRc4H"
      },
      "outputs": [],
      "source": [
        "class ExactNonlin(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(ExactNonlin, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        VT = kb*T/eleccharge\n",
        "        z = z/2 + (VT/2)*torch.log((torch.exp(z/VT)+torch.exp(-z/VT))/(torch.exp(z/VT)+torch.exp(z/VT)))\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RxnCY102Rc6M"
      },
      "outputs": [],
      "source": [
        "class AnaNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, analogL, numpixels, analognumhiddenunits, numclasses):\n",
        "        super(AnaNetwork, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "        self.analogL = analogL\n",
        "        self.activations = []\n",
        "        self.alternate = False  # You can modify this if needed\n",
        "        self.layers = nn.ModuleDict()\n",
        "\n",
        "        self.layers[\"flatten\"] = nn.Flatten()\n",
        "\n",
        "        for i in np.arange(analogL + 1):\n",
        "            if i == 0:\n",
        "                inputsize = numpixels\n",
        "                outputsize = analognumhiddenunits[i]\n",
        "            elif i == analogL:\n",
        "                inputsize = analognumhiddenunits[i - 1]\n",
        "                outputsize = numclasses\n",
        "            else:\n",
        "                inputsize = analognumhiddenunits[i - 1]\n",
        "                outputsize = analognumhiddenunits[i]\n",
        "\n",
        "            red = True if (not self.alternate or i % 2 == 0) else False\n",
        "\n",
        "            if self.params.semianalog:\n",
        "                # Analog pre-processing layer\n",
        "                self.layers[f\"analogprepro{i+1}\"] = ActivationEncoding(self.params, inputsize, outputsize)\n",
        "\n",
        "                # Analog weight encoding and mixing layer\n",
        "                self.layers[f\"analoglayer{i+1}\"] = WeightEncodingandMixing(self.params, inputsize, outputsize)\n",
        "\n",
        "                # A2D conversion layer\n",
        "                self.layers[f\"A2D{i+1}\"] = A2D(self.params, inputsize, outputsize)\n",
        "\n",
        "                # MAFT filter layer\n",
        "                self.layers[f\"filter{i+1}\"] = MAFTFilter(self.params, inputsize, outputsize)\n",
        "\n",
        "                # Non-linearity layer\n",
        "                self.layers[f\"analognonlin{i+1}\"] = MyNonlin()\n",
        "\n",
        "            else:\n",
        "                if i == 0:\n",
        "                    # Analog pre-processing layer with no noise\n",
        "                    self.layers[f\"analogprepro{i+1}\"] = ActivationEncoding(self.params, inputsize, outputsize)\n",
        "                    totaltuple = (1, self.layers[f\"analogprepro{i+1}\"].total)\n",
        "\n",
        "                # Analog weight encoding and mixing layer\n",
        "                self.layers[f\"analoglayer{i+1}\"] = WeightEncodingandMixing(self.params, inputsize, outputsize,\n",
        "                                                                           actshape=totaltuple)\n",
        "\n",
        "                # Dropout layer\n",
        "                # self.layers[f\"dropout{i+1}\"] = nn.Dropout(0.3)\n",
        "\n",
        "                # conv layer\n",
        "                self.layers[f\"conv1d{i+1}\"] = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "                # A2D conversion layer\n",
        "                self.layers[f\"A2D{i+1}\"] = A2D(self.params, inputsize, outputsize)\n",
        "\n",
        "                # MAFT filter timeoutput layer\n",
        "                self.layers[f\"filtertime{i+1}\"] = MAFTFilter_TimeOutput(self.params, inputsize, outputsize)\n",
        "\n",
        "                # MAFT filter layer\n",
        "                self.layers[f\"filter{i+1}\"] = MAFTFilter(self.params, inputsize, outputsize)\n",
        "\n",
        "                # Non-linearity layer\n",
        "                self.layers[f\"analognonlin{i+1}\"] = MyNonlin()\n",
        "                # self.layers[f\"analognonlin{i+1}\"] = SquareNonlin()\n",
        "                # self.layers[f\"analognonlin{i+1}\"] = ExactNonlin()\n",
        "\n",
        "            if self.params.resconn:\n",
        "                self.layers[f\"analogclip{i+1}\"] = MAFTClip(outputsize)\n",
        "                self.layers[f\"analogavg{i+1}\"] = MAFTAvg(inputsize, outputsize)\n",
        "\n",
        "        # Register hooks to save activations\n",
        "        for name, layer in self.layers.items():\n",
        "            if 'analogprepro' in name:\n",
        "                layer.register_forward_hook(self.save_activation)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        device = output[1].device  # Ensure that activations are tracked on the correct device\n",
        "        _, output2 = output\n",
        "        self.activations.append(output2.to(device))  # Move activations to the correct device if necessary\n",
        "\n",
        "    def get_sum_of_squares(self):\n",
        "        sum_of_squares = 0\n",
        "        for activation in self.activations:\n",
        "            sum_of_squares += torch.sum(activation)\n",
        "        return sum_of_squares\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = []  # Clear previous activations\n",
        "        device = x.device  # Extract device from input tensor\n",
        "        x = self.layers[\"flatten\"](x)\n",
        "\n",
        "        for i in np.arange(self.analogL + 1):\n",
        "            if self.params.resconn:\n",
        "                if not self.params.avg:\n",
        "                    xclip = self.layers[f\"analogclip{i+1}\"](x.to(device))  # Ensure xclip is on the correct device\n",
        "                else:\n",
        "                    xclip = self.layers[f\"analogavg{i+1}\"](x.to(device))\n",
        "\n",
        "            if self.params.semianalog:\n",
        "                x, _ = self.layers[f\"analogprepro{i+1}\"](x.to(device))\n",
        "                x = self.layers[f\"analoglayer{i+1}\"](x)\n",
        "                x = self.layers[f\"A2D{i+1}\"](x)\n",
        "                x = self.layers[f\"filter{i+1}\"](x)\n",
        "\n",
        "                if i != self.analogL:\n",
        "                    x = self.layers[f\"analognonlin{i+1}\"](x)\n",
        "\n",
        "                if self.params.resconn:\n",
        "                    x = x + xclip  # Residual connection on the correct device\n",
        "\n",
        "            else:\n",
        "                if i == 0:\n",
        "                    x, _ = self.layers[f\"analogprepro{i+1}\"](x.to(device))\n",
        "                    # print(f\"Energy after actencoding is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "\n",
        "                x = self.layers[f\"analoglayer{i+1}\"](x)\n",
        "                # print(f\"Energy after mixer is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "                x = self.layers[f\"A2D{i+1}\"](x)\n",
        "\n",
        "                x = self.layers[f\"filtertime{i+1}\"](x)\n",
        "                # x = self.layers[f\"filter{i+1}\"](x) # only for 1 layer\n",
        "\n",
        "                # print energy in x\n",
        "                # print(f\"Energy after filter is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "\n",
        "                # print(x.shape)\n",
        "                # x = (self.layers[f\"conv1d{i+1}\"](x.unsqueeze(1))).squeeze()\n",
        "                # print(x.shape)\n",
        "                # x = self.layers[f\"dropout{i+1}\"](x)\n",
        "\n",
        "                if i != self.analogL:\n",
        "                    x = self.layers[f\"analognonlin{i+1}\"](x)\n",
        "                    # print energy in x\n",
        "                    # print(f\"Energy after nonlinearity is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "\n",
        "                if i == self.analogL:\n",
        "                    x = self.layers[f\"A2D{i+1}\"](x)\n",
        "                    x = self.layers[f\"filter{i+1}\"](x)\n",
        "\n",
        "                if self.params.resconn:\n",
        "                    x = x + xclip  # Residual connection on the correct device\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AnaNetwork1layer(nn.Module):\n",
        "\n",
        "    def __init__(self, params: Params, analogL, numpixels, numclasses):\n",
        "        super(AnaNetwork1layer, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "        self.analogL = analogL\n",
        "        self.activations = []\n",
        "        self.alternate = False  # You can modify this if needed\n",
        "        self.layers = nn.ModuleDict()\n",
        "\n",
        "        self.layers[\"flatten\"] = nn.Flatten()\n",
        "\n",
        "        for i in np.arange(analogL + 1):\n",
        "            if analogL==0:\n",
        "                inputsize = numpixels\n",
        "                outputsize = numclasses\n",
        "            else:\n",
        "                if i == 0:\n",
        "                    inputsize = numpixels\n",
        "                    outputsize = analognumhiddenunits[i]\n",
        "                elif i == analogL:\n",
        "                    inputsize = analognumhiddenunits[i - 1]\n",
        "                    outputsize = numclasses\n",
        "                else:\n",
        "                    inputsize = analognumhiddenunits[i - 1]\n",
        "                    outputsize = analognumhiddenunits[i]\n",
        "\n",
        "            red = True if (not self.alternate or i % 2 == 0) else False\n",
        "\n",
        "            if i == 0:\n",
        "                # Analog pre-processing layer with no noise\n",
        "                self.layers[f\"analogprepro{i+1}\"] = ActivationEncoding(self.params, inputsize, outputsize)\n",
        "                totaltuple = (1, self.layers[f\"analogprepro{i+1}\"].total)\n",
        "\n",
        "            # Analog weight encoding and mixing layer\n",
        "            self.layers[f\"analoglayer{i+1}\"] = WeightEncodingandMixing(self.params, inputsize, outputsize,\n",
        "                                                                        actshape=totaltuple)\n",
        "\n",
        "            # Dropout layer\n",
        "            # self.layers[f\"dropout{i+1}\"] = nn.Dropout(0.3)\n",
        "\n",
        "            # conv layer\n",
        "            # self.layers[f\"conv1d{i+1}\"] = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "            # A2D conversion layer\n",
        "            self.layers[f\"A2D{i+1}\"] = A2D(self.params, inputsize, outputsize)\n",
        "\n",
        "            # MAFT filter timeoutput layer\n",
        "            self.layers[f\"filtertime{i+1}\"] = MAFTFilter_TimeOutput(self.params, inputsize, outputsize)\n",
        "\n",
        "            # MAFT filter layer\n",
        "            self.layers[f\"filter{i+1}\"] = MAFTFilter(self.params, inputsize, outputsize)\n",
        "\n",
        "            # Non-linearity layer\n",
        "            self.layers[f\"analognonlin{i+1}\"] = MyNonlin()\n",
        "            # self.layers[f\"analognonlin{i+1}\"] = SquareNonlin()\n",
        "            # self.layers[f\"analognonlin{i+1}\"] = ExactNonlin()\n",
        "\n",
        "        # Register hooks to save activations\n",
        "        for name, layer in self.layers.items():\n",
        "            if 'analogprepro' in name:\n",
        "                layer.register_forward_hook(self.save_activation)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        device = output[1].device  # Ensure that activations are tracked on the correct device\n",
        "        _, output2 = output\n",
        "        self.activations.append(output2.to(device))  # Move activations to the correct device if necessary\n",
        "\n",
        "    def get_sum_of_squares(self):\n",
        "        sum_of_squares = 0\n",
        "        for activation in self.activations:\n",
        "            sum_of_squares += torch.sum(activation)\n",
        "        return sum_of_squares\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = []  # Clear previous activations\n",
        "        device = x.device  # Extract device from input tensor\n",
        "        x = self.layers[\"flatten\"](x)\n",
        "\n",
        "        for i in np.arange(self.analogL + 1):\n",
        "            if self.params.resconn:\n",
        "                if not self.params.avg:\n",
        "                    xclip = self.layers[f\"analogclip{i+1}\"](x.to(device))  # Ensure xclip is on the correct device\n",
        "                else:\n",
        "                    xclip = self.layers[f\"analogavg{i+1}\"](x.to(device))\n",
        "\n",
        "            if self.params.semianalog:\n",
        "                x, _ = self.layers[f\"analogprepro{i+1}\"](x.to(device))\n",
        "                x = self.layers[f\"analoglayer{i+1}\"](x)\n",
        "                x = self.layers[f\"A2D{i+1}\"](x)\n",
        "                x = self.layers[f\"filter{i+1}\"](x)\n",
        "\n",
        "                if i != self.analogL:\n",
        "                    x = self.layers[f\"analognonlin{i+1}\"](x)\n",
        "\n",
        "                if self.params.resconn:\n",
        "                    x = x + xclip  # Residual connection on the correct device\n",
        "\n",
        "            else:\n",
        "                if i == 0:\n",
        "                    x, _ = self.layers[f\"analogprepro{i+1}\"](x.to(device))\n",
        "                    print(f\"Activations after analogprepro {x[0]}\")\n",
        "                    # print(f\"Energy after actencoding is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "\n",
        "                x = self.layers[f\"analoglayer{i+1}\"](x)\n",
        "                print(f\"Activations after analoglayer {x[0]}\")\n",
        "                # print(f\"Energy after mixer is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "                x = self.layers[f\"A2D{i+1}\"](x)\n",
        "                print(f\"Activations after A2D {x[0]}\")\n",
        "                x = self.layers[f\"filter{i+1}\"](x)\n",
        "                print(f\"Activations after filter {x[0]}\")\n",
        "\n",
        "                # print energy in x\n",
        "                # print(f\"Energy after filter is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "\n",
        "                # print(x.shape)\n",
        "                # x = (self.layers[f\"conv1d{i+1}\"](x.unsqueeze(1))).squeeze()\n",
        "                # print(x.shape)\n",
        "                # x = self.layers[f\"dropout{i+1}\"](x)\n",
        "\n",
        "                # if i != self.analogL:\n",
        "                    # x = self.layers[f\"analognonlin{i+1}\"](x)\n",
        "                    # print energy in x\n",
        "                    # print(f\"Energy after nonlinearity is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\")\n",
        "\n",
        "                if i == self.analogL:\n",
        "                    x = self.layers[f\"A2D{i+1}\"](x)\n",
        "                    x = self.layers[f\"filter{i+1}\"](x)\n",
        "\n",
        "                if self.params.resconn:\n",
        "                    x = x + xclip  # Residual connection on the correct device\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "H9k0yUCU6D1G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svp_1VamRc8i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4cea4ntVRnU2"
      },
      "outputs": [],
      "source": [
        "numpixels = 7*7\n",
        "analogL = 2\n",
        "analognumhiddenunits = [32, 16]\n",
        "numclasses = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ecW8o8qDRnXO"
      },
      "outputs": [],
      "source": [
        "noisein = True\n",
        "noiseout = True\n",
        "\n",
        "lowtemp = True\n",
        "\n",
        "noisefactor = torch.tensor(4)\n",
        "\n",
        "preprounits = torch.tensor(1) # current in A\n",
        "wunits = torch.tensor(1) # voltage in V\n",
        "unitconverter = torch.tensor(1) # converts voltage to conductance,\n",
        "                                # and also has a unitless voltage divider like factor\n",
        "\n",
        "kb = 1.38e-23\n",
        "T = 300\n",
        "hbar = 1.05e-34\n",
        "\n",
        "optics = False\n",
        "red = True\n",
        "omegac = 2*torch.pi*1e9 # let's say 5 GHz is the center frequency\n",
        "\n",
        "weightfreqspacing = torch.tensor(1e3) # in Hz, 1 kHz\n",
        "timespan = 1/weightfreqspacing\n",
        "totBW = torch.tensor(1e8) # in Hz, 100 MHz\n",
        "numcomblines = totBW/weightfreqspacing # which is the number of MACs\n",
        "\n",
        "R_trans = torch.tensor(50) # ohms\n",
        "RF_power = torch.tensor(250e-6) # Watts\n",
        "Vref = torch.sqrt(RF_power*R_trans)\n",
        "\n",
        "analogenergyperMAC = RF_power*timespan/numcomblines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wOxuutItRpz1"
      },
      "outputs": [],
      "source": [
        "R_AWG = 50\n",
        "R_w = 50\n",
        "R_out = 50\n",
        "R_A2D = 50\n",
        "\n",
        "k_th_AWG = 4*kb*T*R_AWG\n",
        "k_th_w = 4*kb*T*R_w\n",
        "k_th_out = 4*kb*T*R_out\n",
        "k_th_A2D = 4*kb*T*R_A2D\n",
        "\n",
        "k_th_AWG_lowtemp = 2*(hbar*omegac*np.cosh(hbar*omegac/(2*kb*T))/np.sinh(hbar*omegac/(2*kb*T)))*R_AWG\n",
        "k_th_w_lowtemp = 2*(hbar*omegac*np.cosh(hbar*omegac/(2*kb*T))/np.sinh(hbar*omegac/(2*kb*T)))*R_w\n",
        "k_th_out_lowtemp = 2*(hbar*omegac*np.cosh(hbar*omegac/(2*kb*T))/np.sinh(hbar*omegac/(2*kb*T)))*R_out\n",
        "k_th_A2D_lowtemp = 2*(hbar*omegac*np.cosh(hbar*omegac/(2*kb*T))/np.sinh(hbar*omegac/(2*kb*T)))*R_A2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "1v1_EyI_Rp2L",
        "outputId": "72f1c261-7947-4891-ecec-fb4e11b62619"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'paramsfullyanalog' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-546a7f86c433>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparamsfullyanalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'paramsfullyanalog' is not defined"
          ]
        }
      ],
      "source": [
        "paramsfullyanalog.device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "# numpixels = 28*28\n",
        "\n",
        "analogL = 0\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "# analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "N = numpixels\n",
        "R = numclasses\n",
        "n0 = 0\n",
        "r0 = R * (N - 1) / 2\n",
        "highestindex = int(2 * ((r0 + R) + (n0 + N) * R) + 4)\n",
        "\n",
        "tmax = 1/(totBW/highestindex)\n",
        "tstep = 1/totBW\n",
        "t = torch.arange(0, tmax+tstep, tstep)\n",
        "\n",
        "del_w = (totBW/highestindex)\n",
        "w_carrier = torch.tensor(2*np.pi*0.4e9)\n",
        "w_carrier_wei = torch.tensor(2*np.pi*0.8e9)\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False,\n",
        "                           t=t, del_w=del_w, w_carrier=w_carrier, w_carrier_wei=w_carrier_wei)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    # model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    model = AnaNetwork1layer(paramsfullyanalog, analogL, numpixels, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    energypermaclocal = 1e-14\n",
        "    R_trans = 50\n",
        "    delta_t = 1/paramsfullyanalog.totBW\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "\n",
        "            scalefactlocal = torch.sqrt(energypermaclocal * R_trans / (delta_t))\n",
        "            # outputs = model(scalefactlocal*images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "akvrxrG1P3r8",
        "outputId": "9d10e086-ef4d-47de-8f1e-c210083bdbf6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'k_th_AWG' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4a32d77b13c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n\u001b[1;32m     55\u001b[0m                  \u001b[0mtotBW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotBW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisein\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoisein\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoiseout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoiseout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlowtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                  \u001b[0mweightfreqspacing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweightfreqspacing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_th_AWG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_th_AWG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_th_AWG_lowtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_th_AWG_lowtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                  \u001b[0mk_th_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_th_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_th_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_th_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                  \u001b[0mk_th_w_lowtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_th_w_lowtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_th_out_lowtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_th_out_lowtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'k_th_AWG' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AnaNetwork1layer(paramsfullyanalog, analogL, numpixels, numclasses).to(device)\n",
        "\n",
        "for images, labels in trainloader:\n",
        "    print(model(images[0]))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "pmGA7LhnExH5",
        "outputId": "a4ee7615-1ee2-41b0-c81f-c3ca3b3b71e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-5665034ad5cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnaNetwork1layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamsfullyanalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalogL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "# numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "analognumhiddenunits = [32, 16]\n",
        "# analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    energypermaclocal = 1e-14\n",
        "    R_trans = 50\n",
        "    delta_t = 1/paramsfullyanalog.totBW\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "\n",
        "            scalefactlocal = torch.sqrt(energypermaclocal * R_trans / (delta_t))\n",
        "            # outputs = model(scalefactlocal*images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "60w4mqTn8Gth",
        "outputId": "963aa949-8023-4a9b-ae62-803318c965a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.1857,        Training Accuracy: 61.39%, Validation Accuracy: 61.30%,        Time: 36.92517280578613\n",
            "Epoch [2/100], Loss: 0.8598,        Training Accuracy: 69.78%, Validation Accuracy: 69.27%,        Time: 35.405282855033875\n",
            "Epoch [3/100], Loss: 0.8099,        Training Accuracy: 74.43%, Validation Accuracy: 74.64%,        Time: 35.23499393463135\n",
            "Epoch [4/100], Loss: 1.0763,        Training Accuracy: 78.52%, Validation Accuracy: 78.64%,        Time: 34.78070646524429\n",
            "Epoch [5/100], Loss: 0.6213,        Training Accuracy: 81.33%, Validation Accuracy: 81.43%,        Time: 34.68626856803894\n",
            "Epoch [6/100], Loss: 0.4542,        Training Accuracy: 83.80%, Validation Accuracy: 83.66%,        Time: 34.63933348655701\n",
            "Epoch [7/100], Loss: 0.4990,        Training Accuracy: 84.86%, Validation Accuracy: 84.69%,        Time: 34.44905877113342\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6ac065e6e660>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# print(f\"train acc computed\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n\u001b[1;32m     98\u001b[0m         \u001b[0mTraining\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidation\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-07af2bff2965>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(loader, model, device, printprogress)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Disable gradient calculation for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Move the input and labels to the same device as the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \"\"\"\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m_log_api_usage_once\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpHVJaez8Gxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "# numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    energypermaclocal = 1e-14\n",
        "    R_trans = 50\n",
        "    delta_t = 1/paramsfullyanalog.totBW\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "\n",
        "            scalefactlocal = torch.sqrt(energypermaclocal * R_trans / (delta_t))\n",
        "            outputs = model(scalefactlocal*images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "_ZDOjtTESnDE",
        "outputId": "0d3cc163-12ff-496a-c935-0e622e453860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.3026,        Training Accuracy: 9.61%, Validation Accuracy: 10.36%,        Time: 451.5763108730316\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6c526a25ab9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m# print(f\"batch number is {i}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DqeqLCv3SnFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "# numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    energypermaclocal = 1e-14\n",
        "    R_trans = 50\n",
        "    delta_t = 1/paramsfullyanalog.totBW\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "\n",
        "            scalefactlocal = torch.sqrt(energypermaclocal * R_trans / (delta_t))\n",
        "            outputs = model(scalefactlocal*images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "xB-hatVgSnHj",
        "outputId": "6dbf4fb0-001f-4fb3-d3c7-f0741cdd8d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.3026,        Training Accuracy: 10.26%, Validation Accuracy: 10.00%,        Time: 505.6519498825073\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-6c526a25ab9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m# print(f\"batch number is {i}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "# numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    energypermaclocal = 1e-18\n",
        "    R_trans = 50\n",
        "    delta_t = 1/paramsfullyanalog.totBW\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "\n",
        "            scalefactlocal = torch.sqrt(energypermaclocal * R_trans / (delta_t))\n",
        "            outputs = model(scalefactlocal*images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "U-rLhxrFZrzX",
        "outputId": "5faae435-249c-4a2d-a0d8-b6f66affd23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.3026,        Training Accuracy: 8.25%, Validation Accuracy: 7.98%,        Time: 404.31306409835815\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3110d2b83b9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m# print(f\"batch number is {i}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "# numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "analognumhiddenunits = [32, 16]\n",
        "# analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    energypermaclocal = 1e-18\n",
        "    R_trans = 50\n",
        "    delta_t = 1/paramsfullyanalog.totBW\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "\n",
        "            scalefactlocal = torch.sqrt(energypermaclocal * R_trans / (delta_t))\n",
        "            outputs = model(scalefactlocal*images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B1i-kYqtNaU8",
        "outputId": "e26c2104-d4cc-42aa-91a9-0de4d175bbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Energy after actencoding is 1.4562314307764467e-15\n",
            "Energy after mixer is 1.6052859435317796e-18\n",
            "Energy after filter is 7.472750865494402e-22\n",
            "Energy after nonlinearity is 6.626958367729232e-37\n",
            "Energy after mixer is 8.134649689282709e-40\n",
            "Energy after filter is 1.471363387541058e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.411501950172552e-15\n",
            "Energy after mixer is 1.6045778769274342e-18\n",
            "Energy after filter is 5.739263736151059e-22\n",
            "Energy after nonlinearity is 4.801053464664246e-37\n",
            "Energy after mixer is 5.888031939338589e-40\n",
            "Energy after filter is 1.401298464324817e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4070480391776522e-15\n",
            "Energy after mixer is 1.5706326593349425e-18\n",
            "Energy after filter is 5.310427336040392e-22\n",
            "Energy after nonlinearity is 2.9341604694138825e-37\n",
            "Energy after mixer is 3.613304142200114e-40\n",
            "Energy after filter is 1.0369608636003646e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4338893486058385e-15\n",
            "Energy after mixer is 1.6415316538053579e-18\n",
            "Energy after filter is 8.19468758830685e-22\n",
            "Energy after nonlinearity is 7.050211040048985e-37\n",
            "Energy after mixer is 8.640714616688974e-40\n",
            "Energy after filter is 1.3452465257518244e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4890524751748932e-15\n",
            "Energy after mixer is 1.6421412859168095e-18\n",
            "Energy after filter is 7.591819637262289e-22\n",
            "Energy after nonlinearity is 6.417030405308117e-37\n",
            "Energy after mixer is 8.017487124680511e-40\n",
            "Energy after filter is 1.5414283107572988e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.43036971495176e-15\n",
            "Energy after mixer is 1.6191873341111927e-18\n",
            "Energy after filter is 8.005525558993072e-22\n",
            "Energy after nonlinearity is 6.6514239178775716e-37\n",
            "Energy after mixer is 8.160657788780578e-40\n",
            "Energy after filter is 1.4153114489680652e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4491147658327345e-15\n",
            "Energy after mixer is 1.6445468305356902e-18\n",
            "Energy after filter is 7.379996474040771e-22\n",
            "Energy after nonlinearity is 7.604240956538685e-37\n",
            "Energy after mixer is 9.262905147833836e-40\n",
            "Energy after filter is 1.8637269575520067e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4250346779334788e-15\n",
            "Energy after mixer is 1.6403064959205903e-18\n",
            "Energy after filter is 6.608805043354751e-22\n",
            "Energy after nonlinearity is 5.05113793271007e-37\n",
            "Energy after mixer is 6.2054820934467334e-40\n",
            "Energy after filter is 1.2191296639625909e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4044252016564792e-15\n",
            "Energy after mixer is 1.6021535139496179e-18\n",
            "Energy after filter is 5.732193522956361e-22\n",
            "Energy after nonlinearity is 3.7119492762714608e-37\n",
            "Energy after mixer is 4.570180798564315e-40\n",
            "Energy after filter is 8.547920632381384e-44\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4321202144163795e-15\n",
            "Energy after mixer is 1.6806442686844684e-18\n",
            "Energy after filter is 5.859856713405578e-22\n",
            "Energy after nonlinearity is 3.241064339175641e-37\n",
            "Energy after mixer is 3.96821100425966e-40\n",
            "Energy after filter is 9.949219096706201e-44\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n",
            "Energy after actencoding is 1.4449569987320184e-15\n",
            "Energy after mixer is 1.687113544857669e-18\n",
            "Energy after filter is 7.873287227729064e-22\n",
            "Energy after nonlinearity is 8.363852343924796e-37\n",
            "Energy after mixer is 1.019835595067851e-39\n",
            "Energy after filter is 2.14398665041697e-43\n",
            "Energy after nonlinearity is 0.0\n",
            "Energy after mixer is 0.0\n",
            "Energy after filter is 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3110d2b83b9c>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mscalefactlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergypermaclocal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mR_trans\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalefactlocal\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-38be153098d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"analogprepro{i+1}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Energy after actencoding is {torch.sum(x[0] ** 2) * (1 / self.params.totBW)/50}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"analoglayer{i+1}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "# numpixels = 7*7\n",
        "# numpixels = 14*14\n",
        "numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "analognumhiddenunits = [64, 32]\n",
        "# analognumhiddenunits = [100, 100]\n",
        "# analognumhiddenunits = [50, 50]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = False\n",
        "\n",
        "noiseout = False\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=False, includeoutputfft=False,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "            outputs = model(10*images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "OuIV_Z2dSSBE",
        "outputId": "d20981a9-a643-4978-aabc-f04d0d22ec6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.0889,        Training Accuracy: 64.68%, Validation Accuracy: 65.69%,        Time: 397.21464800834656\n",
            "Epoch [2/100], Loss: 0.2454,        Training Accuracy: 83.28%, Validation Accuracy: 83.56%,        Time: 397.4351716041565\n",
            "Epoch [3/100], Loss: 0.9671,        Training Accuracy: 87.02%, Validation Accuracy: 87.33%,        Time: 397.45677439371747\n",
            "Epoch [4/100], Loss: 0.0739,        Training Accuracy: 88.17%, Validation Accuracy: 88.10%,        Time: 397.60210359096527\n",
            "Epoch [5/100], Loss: 0.1795,        Training Accuracy: 89.39%, Validation Accuracy: 89.40%,        Time: 397.6714231967926\n",
            "Epoch [6/100], Loss: 0.1092,        Training Accuracy: 90.84%, Validation Accuracy: 91.13%,        Time: 397.72291747728985\n",
            "Epoch [7/100], Loss: 0.1690,        Training Accuracy: 91.60%, Validation Accuracy: 91.71%,        Time: 397.775438444955\n",
            "Epoch [8/100], Loss: 0.3167,        Training Accuracy: 92.15%, Validation Accuracy: 92.28%,        Time: 397.7806761562824\n",
            "Epoch [9/100], Loss: 0.2309,        Training Accuracy: 92.47%, Validation Accuracy: 92.39%,        Time: 397.8007962703705\n",
            "Epoch [10/100], Loss: 0.3962,        Training Accuracy: 92.18%, Validation Accuracy: 91.93%,        Time: 397.8230744123459\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e6988300fd1b>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Calculate training and validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# print(f\"epoch {epoch} done\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# print(f\"train acc computed\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-07af2bff2965>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(loader, model, device, printprogress)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Calculate total and correct predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprintprogress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkjUtgYc6dGR",
        "outputId": "9b6e2ddd-4c5f-496c-bfdb-5f73b6dccd18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.7079,        Training Accuracy: 56.25%, Validation Accuracy: 56.14%,        Time: 179.96079754829407\n",
            "Epoch [2/100], Loss: 1.2817,        Training Accuracy: 61.55%, Validation Accuracy: 61.38%,        Time: 179.14620971679688\n",
            "Epoch [3/100], Loss: 1.5885,        Training Accuracy: 64.10%, Validation Accuracy: 64.09%,        Time: 178.93843817710876\n",
            "Epoch [4/100], Loss: 1.2135,        Training Accuracy: 66.20%, Validation Accuracy: 66.09%,        Time: 178.8510321378708\n",
            "Epoch [5/100], Loss: 1.6259,        Training Accuracy: 67.20%, Validation Accuracy: 67.06%,        Time: 178.75732870101928\n",
            "Epoch [6/100], Loss: 0.9910,        Training Accuracy: 68.14%, Validation Accuracy: 67.89%,        Time: 178.63926756381989\n",
            "Epoch [7/100], Loss: 0.8932,        Training Accuracy: 67.47%, Validation Accuracy: 67.13%,        Time: 178.6349607535771\n",
            "Epoch [8/100], Loss: 0.9440,        Training Accuracy: 70.94%, Validation Accuracy: 70.45%,        Time: 178.59112191200256\n",
            "Epoch [9/100], Loss: 0.7990,        Training Accuracy: 70.41%, Validation Accuracy: 70.09%,        Time: 178.58614105648465\n",
            "Epoch [10/100], Loss: 0.9359,        Training Accuracy: 70.75%, Validation Accuracy: 69.76%,        Time: 178.58686630725862\n",
            "Epoch [11/100], Loss: 1.0280,        Training Accuracy: 71.38%, Validation Accuracy: 71.18%,        Time: 178.57525567574933\n",
            "Epoch [12/100], Loss: 0.9964,        Training Accuracy: 71.51%, Validation Accuracy: 71.26%,        Time: 178.53099473317465\n",
            "Epoch [13/100], Loss: 1.3106,        Training Accuracy: 71.58%, Validation Accuracy: 71.13%,        Time: 178.50282843296344\n",
            "Epoch [14/100], Loss: 0.8181,        Training Accuracy: 73.77%, Validation Accuracy: 72.88%,        Time: 178.51666527135032\n",
            "Epoch [15/100], Loss: 1.1906,        Training Accuracy: 71.64%, Validation Accuracy: 71.20%,        Time: 178.50393578211467\n",
            "Epoch [16/100], Loss: 1.2297,        Training Accuracy: 73.74%, Validation Accuracy: 72.93%,        Time: 178.4999267309904\n",
            "Epoch [17/100], Loss: 0.8788,        Training Accuracy: 73.63%, Validation Accuracy: 72.81%,        Time: 178.48070952471565\n",
            "Epoch [18/100], Loss: 0.6877,        Training Accuracy: 74.53%, Validation Accuracy: 74.59%,        Time: 178.48095571994781\n",
            "Epoch [19/100], Loss: 0.7418,        Training Accuracy: 73.09%, Validation Accuracy: 72.57%,        Time: 178.4766319049032\n",
            "Epoch [20/100], Loss: 0.5975,        Training Accuracy: 74.76%, Validation Accuracy: 74.75%,        Time: 178.4905390739441\n",
            "Epoch [21/100], Loss: 1.3830,        Training Accuracy: 75.19%, Validation Accuracy: 74.34%,        Time: 178.4797855445317\n",
            "Epoch [22/100], Loss: 0.5234,        Training Accuracy: 75.00%, Validation Accuracy: 74.84%,        Time: 178.47082909670743\n",
            "Epoch [23/100], Loss: 0.8330,        Training Accuracy: 74.26%, Validation Accuracy: 73.73%,        Time: 178.46440093413642\n",
            "Epoch [24/100], Loss: 0.7955,        Training Accuracy: 73.28%, Validation Accuracy: 73.10%,        Time: 178.4525756239891\n",
            "Epoch [25/100], Loss: 0.8334,        Training Accuracy: 75.56%, Validation Accuracy: 75.03%,        Time: 178.4423627090454\n",
            "Epoch [26/100], Loss: 0.9416,        Training Accuracy: 76.07%, Validation Accuracy: 75.77%,        Time: 178.42820059336148\n",
            "Epoch [27/100], Loss: 0.4721,        Training Accuracy: 76.48%, Validation Accuracy: 75.93%,        Time: 178.42319839089006\n",
            "Epoch [28/100], Loss: 0.7367,        Training Accuracy: 73.98%, Validation Accuracy: 73.80%,        Time: 178.42601962600435\n",
            "Epoch [29/100], Loss: 0.7606,        Training Accuracy: 76.34%, Validation Accuracy: 76.12%,        Time: 178.42573692058696\n",
            "Epoch [30/100], Loss: 0.8469,        Training Accuracy: 76.23%, Validation Accuracy: 76.17%,        Time: 178.4234625418981\n",
            "Epoch [31/100], Loss: 0.8302,        Training Accuracy: 77.05%, Validation Accuracy: 76.42%,        Time: 178.42219475007826\n",
            "Epoch [32/100], Loss: 0.7717,        Training Accuracy: 77.28%, Validation Accuracy: 76.99%,        Time: 178.4173288717866\n",
            "Epoch [33/100], Loss: 0.9712,        Training Accuracy: 76.57%, Validation Accuracy: 76.26%,        Time: 178.40994015606967\n",
            "Epoch [34/100], Loss: 0.8023,        Training Accuracy: 77.10%, Validation Accuracy: 76.76%,        Time: 178.40563610020806\n",
            "Epoch [35/100], Loss: 0.4481,        Training Accuracy: 76.70%, Validation Accuracy: 76.09%,        Time: 178.39831478255135\n",
            "Epoch [36/100], Loss: 1.1203,        Training Accuracy: 75.52%, Validation Accuracy: 75.24%,        Time: 178.38791942596436\n",
            "Epoch [37/100], Loss: 0.8623,        Training Accuracy: 77.64%, Validation Accuracy: 77.11%,        Time: 178.38413276543488\n",
            "Epoch [38/100], Loss: 0.8478,        Training Accuracy: 77.30%, Validation Accuracy: 76.81%,        Time: 178.38852645848928\n",
            "Epoch [39/100], Loss: 0.9555,        Training Accuracy: 77.06%, Validation Accuracy: 76.68%,        Time: 178.38946742277878\n",
            "Epoch [40/100], Loss: 0.6068,        Training Accuracy: 76.63%, Validation Accuracy: 75.92%,        Time: 178.38137303590776\n",
            "Epoch [41/100], Loss: 0.5626,        Training Accuracy: 77.05%, Validation Accuracy: 76.54%,        Time: 178.37599795620616\n",
            "Epoch [42/100], Loss: 1.2715,        Training Accuracy: 76.84%, Validation Accuracy: 76.75%,        Time: 178.3666612761361\n",
            "Epoch [43/100], Loss: 0.4844,        Training Accuracy: 77.34%, Validation Accuracy: 77.32%,        Time: 178.36245379891506\n",
            "Epoch [44/100], Loss: 1.7724,        Training Accuracy: 77.54%, Validation Accuracy: 77.05%,        Time: 178.36107332056218\n",
            "Epoch [45/100], Loss: 1.2218,        Training Accuracy: 78.07%, Validation Accuracy: 77.51%,        Time: 178.3393524646759\n",
            "Epoch [46/100], Loss: 1.4692,        Training Accuracy: 77.05%, Validation Accuracy: 76.28%,        Time: 178.31632018089294\n",
            "Epoch [47/100], Loss: 0.7196,        Training Accuracy: 77.69%, Validation Accuracy: 77.43%,        Time: 178.29809599227093\n",
            "Epoch [48/100], Loss: 1.0355,        Training Accuracy: 77.38%, Validation Accuracy: 77.04%,        Time: 178.28329467773438\n",
            "Epoch [49/100], Loss: 1.3750,        Training Accuracy: 77.60%, Validation Accuracy: 77.77%,        Time: 178.2633851353003\n",
            "Epoch [50/100], Loss: 1.2022,        Training Accuracy: 78.90%, Validation Accuracy: 78.49%,        Time: 178.25412099838258\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "# numpixels = 7*7\n",
        "numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "analognumhiddenunits = [100, 100]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = True\n",
        "\n",
        "noiseout = True\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=True, includeoutputfft=True,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_PSQVudfMjW",
        "outputId": "ecbd2df6-b4f2-49c6-dccd-a781bd7a667b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.3796,        Training Accuracy: 58.43%, Validation Accuracy: 58.50%,        Time: 173.70931029319763\n",
            "Epoch [2/20], Loss: 1.3173,        Training Accuracy: 63.90%, Validation Accuracy: 63.81%,        Time: 172.87598156929016\n",
            "Epoch [3/20], Loss: 1.2174,        Training Accuracy: 66.23%, Validation Accuracy: 65.58%,        Time: 172.73610790570578\n",
            "Epoch [4/20], Loss: 0.6843,        Training Accuracy: 67.48%, Validation Accuracy: 67.02%,        Time: 172.6817905306816\n",
            "Epoch [5/20], Loss: 0.8731,        Training Accuracy: 68.90%, Validation Accuracy: 69.05%,        Time: 172.65811524391174\n",
            "Epoch [6/20], Loss: 1.4568,        Training Accuracy: 67.61%, Validation Accuracy: 66.85%,        Time: 172.65196216106415\n",
            "Epoch [7/20], Loss: 1.3731,        Training Accuracy: 72.10%, Validation Accuracy: 71.81%,        Time: 172.6358506679535\n",
            "Epoch [8/20], Loss: 0.5707,        Training Accuracy: 70.84%, Validation Accuracy: 70.06%,        Time: 172.64169937372208\n",
            "Epoch [9/20], Loss: 0.5899,        Training Accuracy: 70.99%, Validation Accuracy: 70.31%,        Time: 172.58503251605563\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "# numpixels = 7*7\n",
        "numpixels = 28*28\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [32, 16]\n",
        "analognumhiddenunits = [100, 100]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = True\n",
        "\n",
        "noiseout = True\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=True,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=True, includeoutputfft=True,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 20\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yq-7HFRRxqn",
        "outputId": "9998d8e7-578b-4dbb-bdfd-55006b92367c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.6674,        Training Accuracy: 73.17%, Validation Accuracy: 72.78%,        Time: 32.64045715332031\n",
            "Epoch [2/20], Loss: 0.8640,        Training Accuracy: 77.14%, Validation Accuracy: 77.37%,        Time: 31.538044452667236\n",
            "Epoch [3/20], Loss: 0.3863,        Training Accuracy: 76.74%, Validation Accuracy: 76.59%,        Time: 31.66091235478719\n",
            "Epoch [4/20], Loss: 0.9778,        Training Accuracy: 73.14%, Validation Accuracy: 73.44%,        Time: 31.344755828380585\n",
            "Epoch [5/20], Loss: 1.0761,        Training Accuracy: 78.97%, Validation Accuracy: 79.40%,        Time: 31.312420463562013\n",
            "Epoch [6/20], Loss: 0.4027,        Training Accuracy: 75.48%, Validation Accuracy: 75.08%,        Time: 31.181117177009583\n",
            "Epoch [7/20], Loss: 0.1805,        Training Accuracy: 76.18%, Validation Accuracy: 76.79%,        Time: 31.117038556507655\n",
            "Epoch [8/20], Loss: 0.7731,        Training Accuracy: 79.66%, Validation Accuracy: 79.90%,        Time: 31.01620301604271\n",
            "Epoch [9/20], Loss: 0.6179,        Training Accuracy: 78.96%, Validation Accuracy: 79.43%,        Time: 31.026342550913494\n",
            "Epoch [10/20], Loss: 0.4330,        Training Accuracy: 72.15%, Validation Accuracy: 72.11%,        Time: 31.05586874485016\n",
            "Epoch [11/20], Loss: 1.0280,        Training Accuracy: 74.60%, Validation Accuracy: 74.93%,        Time: 30.995696024461225\n",
            "Epoch [12/20], Loss: 0.4319,        Training Accuracy: 80.31%, Validation Accuracy: 81.15%,        Time: 31.00957198937734\n",
            "Epoch [13/20], Loss: 0.5921,        Training Accuracy: 80.67%, Validation Accuracy: 80.68%,        Time: 30.953931386654194\n",
            "Epoch [14/20], Loss: 0.3731,        Training Accuracy: 78.82%, Validation Accuracy: 78.94%,        Time: 31.0099378994533\n",
            "Epoch [15/20], Loss: 0.4028,        Training Accuracy: 77.23%, Validation Accuracy: 77.80%,        Time: 30.995807870229086\n",
            "Epoch [16/20], Loss: 0.3012,        Training Accuracy: 76.84%, Validation Accuracy: 77.28%,        Time: 31.037608802318573\n",
            "Epoch [17/20], Loss: 0.6620,        Training Accuracy: 79.12%, Validation Accuracy: 79.35%,        Time: 31.005423209246466\n",
            "Epoch [18/20], Loss: 0.1393,        Training Accuracy: 80.73%, Validation Accuracy: 81.11%,        Time: 31.013187024328445\n",
            "Epoch [19/20], Loss: 1.1720,        Training Accuracy: 81.11%, Validation Accuracy: 81.20%,        Time: 30.98307728767395\n",
            "Epoch [20/20], Loss: 0.7111,        Training Accuracy: 79.68%, Validation Accuracy: 79.93%,        Time: 30.936908292770386\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "\n",
        "# Looping over model training\n",
        "\n",
        "# inputsize = 784\n",
        "# hiddenunits = [100, 100]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 49\n",
        "# hiddenunits = [16, 16]\n",
        "# outputsize = 10\n",
        "\n",
        "# inputsize = 196\n",
        "# hiddenunits = [64, 32]\n",
        "# outputsize = 10\n",
        "\n",
        "numpixels = 7*7\n",
        "\n",
        "analogL = 2\n",
        "\n",
        "analognumhiddenunits = [32, 16]\n",
        "\n",
        "# numpixels = 10*10\n",
        "\n",
        "# analogL = 2\n",
        "\n",
        "# analognumhiddenunits = [64, 32]\n",
        "\n",
        "numclasses = 10\n",
        "\n",
        "noisein = True\n",
        "\n",
        "noiseout = True\n",
        "\n",
        "paramsfullyanalog = Params(red=red, units=preprounits, fixedband=False,\n",
        "                 totBW=totBW, noisein=noisein, noiseout=noiseout, lowtemp=lowtemp,\n",
        "                 weightfreqspacing=weightfreqspacing, k_th_AWG=k_th_AWG, k_th_AWG_lowtemp=k_th_AWG_lowtemp,\n",
        "                 k_th_w=k_th_w, k_th_out=k_th_out,\n",
        "                 k_th_w_lowtemp=k_th_w_lowtemp, k_th_out_lowtemp=k_th_out_lowtemp,\n",
        "                 k_th_A2D=k_th_A2D, k_th_A2D_lowtemp=k_th_A2D_lowtemp,\n",
        "                 digitallike=False, noisefactor=noisefactor, optics=optics,\n",
        "                 fixedpowerflag=False, fixedpower=None,\n",
        "                 includeinputfft=True, includeoutputfft=True,\n",
        "                 scaletoz=False, scaletoW=False, wunits=wunits, unitconverter=unitconverter,\n",
        "                 photodiodearea=1e-6, omegaoptical=omegac,\n",
        "                 inttime=1e-7, capval=1e-12, semianalog=False, resconn=False)\n",
        "\n",
        "for modelno in range(6, 7):\n",
        "\n",
        "    model = AnaNetwork(paramsfullyanalog, analogL, numpixels, analognumhiddenunits, numclasses).to(device)\n",
        "\n",
        "    num_epochs = 20\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "#     criterion = custom_cross_entropy_loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Forward pass\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # print(f\"batch number is {i}\")\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate training and validation accuracy\n",
        "        # print(f\"epoch {epoch} done\")\n",
        "        train_accuracy = calculate_accuracy(trainloader, model, device)\n",
        "        # print(f\"train acc computed\")\n",
        "        val_accuracy = calculate_accuracy(validloader, model, device)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f},\\\n",
        "        Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%,\\\n",
        "        Time: {(time.time()-start)/(epoch+1)}')\n",
        "\n",
        "#     torch.save(model.state_dict(),\n",
        "#                f\"DigiModel/{inputsize}_{hiddenunits[0]}_{hiddenunits[1]}_{outputsize}_{num_epochs}epo/run{modelno}.pth\")\n",
        "\n",
        "#     print(f\"Saved model number {modelno}\")\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRtvojsRTeeq",
        "outputId": "33d57d49-aeaa-49be-c82e-f69349023b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(next(model.parameters()).is_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV3YlA2WTfJL",
        "outputId": "09359857-06cd-45ba-8b45-e94ed214bc3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Oct  2 17:14:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Gtzf9_OkNc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}